<!DOCTYPE html>
<html>
  <head>
    <title>Getting back into R - and how to handle messy data</title>
    <meta charset="utf-8">
    <meta name="author" content="Matt Craddock" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="css\my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Getting back into R - and how to handle messy data
## Advanced Research Methods and Skills
### Matt Craddock
### 2019/02/19

---




# Topics previously covered

.large[

- Data visualization
    - Using **ggplot2**
    
- Data manipulation
    - Using **dplyr**

- Basic statistics
    - t-tests, correlations
    - regression
    - ANOVA
]

---
# Themes for today

.pull-left[
.large[
- Refamiliarizing yourself with R

- Basic data transformations

- Missing data
]

]
.pull-right[
![:scale 70%](images/RStudioCloud_proj_circ.png)
![:scale 70%](images/tidy-1.png)
]


---
class: center, inverse, middle
# For the new class members...

Join the PSY9219M workspace on RStudio.cloud.
# [https://tinyurl.com/RStudioPSY9219M](https://tinyurl.com/RStudioPSY9219M)

---
# Tabular data

Tables of data are what you're most commonly dealing with in R.
.pull-left[

```
## # A tibble: 16 x 4
##    Participant Viewpoint Block     RT
##          &lt;int&gt; &lt;fct&gt;     &lt;chr&gt;  &lt;dbl&gt;
##  1           1 Different First   492.
##  2           1 Different Second  431.
##  3           1 Same      First   486.
##  4           1 Same      Second  392.
##  5           2 Different First   477.
##  6           2 Different Second  368.
##  7           2 Same      First   536.
##  8           2 Same      Second  418.
##  9           3 Different First   463.
## 10           3 Different Second  390.
## 11           3 Same      First   466.
## 12           3 Same      Second  443.
## 13           4 Different First   457.
## 14           4 Different Second  442.
## 15           4 Same      First   492.
## 16           4 Same      Second  483.
```
]
.pull-right[
This one confirms to the **tidy data** principles:

One row per observation, one column per variable

![:scale 80%](images/tidy-1.png)
]

---
# Different types of file

Data comes in many different shapes, sizes, and formats.

The most common file formats you'll deal with are either Excel files or text files, but you may also find dealing with SPSS files useful.

Fortunately, R has several functions and packages for importing data!

|File formats| File extension| Functions| Package|
|-|-|-|-|
|SPSS  | .sav| **read_sav()**| library(haven)|
|Excel | .xls, .xlsx|**read_excel()**|library(readxl)|
|Text  | .csv, .txt, .* |**read_csv()**, **read_delim()**|library(readr)|


---
background-image: url(../images/dplyr-logo.png)
background-size: 6%
background-position: 90% 5%
# The five verbs of dplyr

**dplyr** is a really useful package for manipulation of data tables.

|Function |Effect|
|------------|----|
| filter()   |Include or exclude observations (rows)|
| select()   |Include or exclude variables (columns)|
| mutate()   |Create new variables (columns)|
| summarise()|Aggregate or summarise groups of observations (rows)|
| arrange()  |Change the order of observations (rows)|

---
background-image: url(../images/ggplot2-logo.png)
background-size: 8%
background-position: 85% 5%
# Quickly plotting your data

Getting plots of your data can be really important, for all aspects of data analysis.

The **ggplot2** package provides a framework for doing this:
.pull-left[

```r
ggplot(example_rt_df,
       aes(x = RT, fill = Block)) +
  geom_density(alpha = 0.5) +
  theme_bw()
```
]
.pull-right[
![](01-messy-data_files/figure-html/dens-plot-1.png)
]

---
# Running statistics

The humble t-test...


```r
t.test(RT~Block, data = example_rt_df)
```

```
## 
## 	Welch Two Sample t-test
## 
## data:  RT by Block
## t = 14.654, df = 195.15, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##   84.15263 110.32544
## sample estimates:
##  mean in group First mean in group Second 
##             500.2523             403.0133
```

---
# Running statistics

The factorial ANOVA... (the aov_ez() function from the *afex* package)


```r
aov_ez(dv = "RT",
       within = c("Block", "Viewpoint"),
       id = "Participant",
       data = example_rt_df)
```

```
## Anova Table (Type 3 tests)
## 
## Response: RT
##            Effect    df     MSE          F    ges p.value
## 1           Block 1, 49 1930.59 244.88 ***    .52  &lt;.0001
## 2       Viewpoint 1, 49 2356.96       0.06  .0003     .81
## 3 Block:Viewpoint 1, 49 1879.71       0.00 &lt;.0001     .97
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1
```

---
# Reminder! 

Practice materials are available on Datacamp and on RStudio.cloud!

.pull-left[
![:scale 95%](../images/Datacamp_.png)
]
.pull-right[
![:scale 50%](../images/rstudio-primers.png)
]

---
class: center, inverse, middle
# How to handle "messy" or otherwise awkward data

---
# The ideal data

In an ideal world all our data would be beautifully normal:

![](01-messy-data_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;


---
# The real data

But reality is rarely so kind. Data can be all kinds of messy. There can be *outliers*...

![](01-messy-data_files/figure-html/outlier-plot-1.png)&lt;!-- --&gt;

---
# The real data

There can be *missing data*...
.pull-left[
![](01-messy-data_files/figure-html/missing-plot-1.png)&lt;!-- --&gt;
]
.pull-right[

```
##           X1        X2
## 1         NA 11.584253
## 2         NA  9.344559
## 3         NA  8.797245
## 4         NA 11.348822
## 5         NA  9.961144
## 6   7.978535        NA
## 7   8.709296        NA
## 8   8.851003  8.887795
## 9   9.630267 10.422302
## 10 11.153628  9.704338
```
Complete cases = 193
]

---
# The real data

Data can *skewed*...

![](01-messy-data_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

---
# The real data


.pull-left[
There can be any combination of these things...

![](01-messy-data_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]
.pull-right[
.large[
All of these pose problems for estimating the properties of our data, the relationships between our variables, and the phenomena we are investigating.
]
]

---
class: center, inverse, middle
# Handling outliers

---
# What is an outlier?

.pull-left[
.large[
Unusual observations that lie outside the typical range of the data.

The data in the figure on the right-hand side was generated with a correlation coefficient (r) of .2.

Two out of the 200 pairs of x-y values were replaced. 

The resulting coefficient (approx r = .5) is *way-off* the true coefficient for these data. 
]
]

.pull-right[
![](01-messy-data_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
]

---
# What is an outlier?

.pull-left[
.large[
The problem gets even worse with smaller sample sizes.

Here there are 50 datapoints with two outliers, rather than 200.

The correlation coefficient becomes even *more* biased than it was previously.

And outliers can *skew* the distribution of the data, causing problems with many standard parametric tests.
]
]

.pull-right[
![](01-messy-data_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;
]

---
# What should we do about them?

There are three typical strategies.

1. Remove them.
    - If you're really sure these reflect an error and not genuine data, then removal is a possibility. 
    - For example, if somebody's age was written down as 1800 years old, you can be resonably sure it's a mistake.
    - Sometimes cutoffs are used (e.g. anything more than 3 standard deviations above the mean is removed)

2. Replace them.
    - Common approaches involve replacing the outliers with values of the mean of the distribution + or - 2 or 3 standard deviations. 

3. Transform the data
    - There are a variety of different ways you can *rescale* or *transform* your data that may help reduce the influence of outliers. (We'll come back to this!)

---
# Removing values above a threshold

The **filter()** function from dplyr can be used to remove outliers easily!

.pull-left[

```r
temp_df_out %&gt;%
  ggplot(aes(x = X1, y = X2)) + 
  geom_point() +
  theme_bw() + 
  stat_smooth(method = "lm")
```

![](01-messy-data_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;
]
.pull-right[

```r
temp_df_out %&gt;%
  filter(X1 &lt; 15) %&gt;%
  ggplot(aes(x = X1, y = X2)) + 
  geom_point() + theme_bw() + 
  stat_smooth(method = "lm")
```

![](01-messy-data_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;
]

---
# Removing values above a threshold

One way to check which values lie several standard deviations away from the mean is to **scale** them. 


```r
temp_df_out %&gt;%
  mutate(X1_scaled = scale(X1),
         X2_scaled = scale(X2)) %&gt;%
  head()
```

```
##          X1        X2  X1_scaled  X2_scaled
## 1 10.266517 11.584253  0.1880828  0.8165705
## 2  9.693975  9.344559 -0.3195414 -0.4572089
## 3 10.901020  8.797245  0.7506418 -0.7684827
## 4  9.715839 11.348822 -0.3001565  0.6826742
## 5 10.926492  9.961144  0.7732261 -0.1065394
## 6  7.978535  9.239321 -1.8404736 -0.5170613
```

---
# Scaling by the mean and standard deviation

The **scale()** function centres a vector on its mean and *standardizes* it. This technique is used frequently in multiple regression involving *interactions* (i.e. moderation and mediation).

.pull-left[

```r
example &lt;- rnorm(10, 0, 2)
scale(example)
```

```
##             [,1]
##  [1,] -1.6144779
##  [2,] -0.6986313
##  [3,]  0.4546991
##  [4,]  0.7995674
##  [5,]  0.3264190
##  [6,] -1.1152730
##  [7,]  1.7144020
##  [8,] -0.6325011
##  [9,]  0.1873481
## [10,]  0.5784476
## attr(,"scaled:center")
## [1] -0.5469003
## attr(,"scaled:scale")
## [1] 1.977476
```

]
.pull-right[

```r
(example - mean(example)) / sd(example)
```

```
##             [,1]
##  [1,] -1.6144779
##  [2,] -0.6986313
##  [3,]  0.4546991
##  [4,]  0.7995674
##  [5,]  0.3264190
##  [6,] -1.1152730
##  [7,]  1.7144020
##  [8,] -0.6325011
##  [9,]  0.1873481
## [10,]  0.5784476
```
]

---
class: center, inverse, middle
# Handling skew

---
# Skewed data

Skewed data is data that *leans* in a particular direction. 

These are often described by the direction of the "long-tail" - so a left-skewed distribution means a distribution with long tail on the left, rather than most values on the left.

![](01-messy-data_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;

---
# Skewed data

A plot of some randomly generated data suggest that it is right-tailed. This is sometimes called *positively skewed*. For this type of data, the mean (blue line) is higher than the median (red, dashed line).

![](01-messy-data_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;

---
# Transformation of skewed data

One way to handle this kind of skew is to transform the data onto a different *scale*.

Transformation type| code|
-------|---|
Log|log(X)|
Square root|sqrt(X)|
Reciprocal | 1 / X|

(See Section 5.8.2 in Field et al., DSUR)
---
# Transformation of skewed data

The different transformations each have different effects on the data, but the general pattern is that they (mostly) reduce the skew, bringing extreme values closer to the centre of the distribution.

![](01-messy-data_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;

---
# Left-tailed skew

Suppose instead that the data is skewed the opposite way - many high scores but few low scores. Thus it has a long *left* tail. This is also called *negative skew*. The mean (blue solid line) is lower than the median (red dashed line).

![](01-messy-data_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;

---
# Transformations on left-tailed data

The transformations aren't as effective on left-tailed data.

![](01-messy-data_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;

---
class: center, inverse, middle
# Handling missing data

---
# "Not Available"

The value NA is code for `Not Available` or Missing Values.

.pull-left[

```r
head(temp_df_missing)
```

```
##         X1        X2
## 1       NA 11.584253
## 2       NA  9.344559
## 3       NA  8.797245
## 4       NA 11.348822
## 5       NA  9.961144
## 6 7.978535        NA
```
]
.pull-right[

```r
summary(temp_df_missing)
```

```
##        X1               X2        
##  Min.   : 7.343   Min.   : 6.783  
##  1st Qu.: 9.312   1st Qu.: 9.373  
##  Median :10.084   Median : 9.934  
##  Mean   : 9.992   Mean   :10.005  
##  3rd Qu.:10.703   3rd Qu.:10.677  
##  Max.   :12.410   Max.   :12.515  
##  NA's   :5        NA's   :2
```
]

---
# Types of missing data

- Missing Completely At Random
    - Missingness does not depend on anything

- Missing At Random
    - Missingness depends on the observed data

- Missing Not At Random
    - Missingness depends on the missing data
    
---
# Missing Completely At Random

.large[
If you have missing data, MCAR is the best kind of missing data. 

There is nothing *systematic* about which data is missing. 

For example, all your participants filled out three different questionnaires. Unfortunately, your dog chewed through a pile of them, and half of your participants now have only two questionnaires.


```
##           X1        X2       X3
## 1         NA  7.855775 12.47424
## 2         NA  9.364811 13.96545
## 3         NA  9.978000 13.74201
## 4         NA  9.598196 11.56024
## 5         NA  9.274233 14.48495
## 6  11.428760  9.046478 14.06569
## 7  10.463065  6.945478 12.92688
## 8  11.130153  9.578408 14.66327
## 9  10.701876 11.352578 13.63511
## 10  7.818268  8.724203 12.26248
```

]

---
# Missing At Random

.large[
Confusingly, Missing At Random (MAR) data is not missing (completely) at random. 

For example, suppose that you collected three questionnaires from participants with a range of ages. However, for some reason, people older than 21 typically failed to complete the third questionnaire.
]
.pull-left[

```
##           X1       X2       X3 age
## 1   8.807741 8.945110 12.50870  20
## 2   8.209651 8.944879 11.92521  20
## 3  10.241487 9.030273       NA  46
## 4  10.350369 9.140197 12.19645  19
## 5   9.391321 9.766557 12.74327  19
## 6   9.779043 9.520460 12.05594  19
## 7  10.546600 9.669037 14.78436  18
## 8   9.201295 8.171754 15.13085  19
## 9  10.050082 8.803442 11.68008  19
## 10 10.404716 9.286622 12.78480  19
```
]
.pull-right[
In this case, the failure to complete the third questionnaire is not related to the responses on the third questionnaire: it is not the case, for example, that people who would have high scores on this questionnaire typically do not complete it. 

So this data is MAR - whether the data in the third column is missing is related to the value of the fourth column.
]

---
# Missing Not At Random

The final, most troubling type of missing data is data that is Missing Not At Random (MNAR).

.pull-left[

```
##           X1        X2       X3 age
## 1   8.946573  8.396240 11.57419  24
## 2  10.583826  9.400410       NA  19
## 3  10.310628 11.357849       NA  19
## 4   9.362511  8.027435 13.32993  19
## 5   9.848544  9.442694 11.23859  19
## 6   9.466695  9.740414 12.22437  19
## 7   9.104667 11.007958 13.15820  20
## 8  10.364775 10.441778       NA  19
## 9  10.341478  8.073513 11.69393  66
## 10  9.444430  8.774358 13.45299  18
```
]
.pull-right[
In this case, the values that are missing for the third questionnaire depends on the value of the responses to that questionnaire, so this data is MNAR.

For example, imagine that the questionnaire relates to depression; people who score high for depression are less likely to complete the questionnaire.

]


---
# Simple methods of dealing with missing data

.large[
List-wise deletion: Cases with missing data are completely **removed** from **all** analysis.

Pair-wise deletion: Cases with missing data are only **removed** from **comparisons where one or more variables are missing**.

Many R functions have built-in methods of dealing with missing values that enforce the methods.

You'll have encountered this before - by default, functions such as **mean()** return NA if any value in the input is NA/missing.


```r
mean(temp_df_missing$X1)
```

```
## [1] NA
```

```r
mean(temp_df_missing$X1, na.rm = TRUE)
```

```
## [1] 9.992288
```
]

---
# Problems with deletion

Deletion can be wasteful. A lot of data is just thrown out. In addition, if the data is Missing Not At Random, this can introduce a lot of bias, since the data is not representative of the full range of the variable of interest!

![](01-messy-data_files/figure-html/final_cors-1.png)&lt;!-- --&gt;

```
##     X1 X3 X2   
## 157  1  1  1  0
## 21   1  1  0  1
## 11   1  0  1  1
## 11   0  1  1  1
##     11 11 21 43
```

---
# Imputation

Imputation is a method of "filling-in" missing values with "best-guesses".

## Single Imputation
Replace missing values with a simple "best-guess". e.g. Using the mean or the median for the condition.

Problem: the mean and median are biased by the missing data. And replacing a missing value with one of these values tends to artificially reduce variability.

## Multiple Imputation
Replace missing values with estimates based on *model* of the data that incorporates uncertainty. We create a model based on the data that is not missing, and use its predictions to guess the values that the missing data has.

Packages such as **mice** and **Amelia** can do this for us.

---
# Alternative approaches to missing data, skew, and other oddities

.large[
Generalized Linear Models (as opposed to General Linear Models) allow modelling of data of many different types without necessitating transformations. For example, counts can be modelled using Poisson regression, and categorical outcomes with logistic regression.

Multilevel, hierarchial models can handle all of these things and much more besides; they are perfectly capable of handling missing data.

We'll cover both of these later in the course.
]

---
# Next week

Look into power and effect sizes:

See Field et al, Discovering Statistics Using R, pages 56-59, Sections on:
    - Type I and Type II error (2.6.3)
    - effect sizes (2.6.4)
    - statistical power (2.6.5)

Cohen, J. (1992). A power primer. Psychological Bulletin, 112(1), 155-159.
http://dx.doi.org/10.1037/0033-2909.112.1.155
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="js/macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
