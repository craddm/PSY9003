<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Getting back into R - and how to handle messy data</title>
    <meta charset="utf-8" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="css\my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Getting back into R - and how to handle messy data
## Advanced Research Methods and Skills
### 2020/02/18

---




# Topics previously covered

- Data visualization
    - Using **ggplot2**
    
- Data manipulation
    - Using **dplyr**

- Basic statistics
    - t-tests, correlations
    - regression
    - ANOVA

---
# Themes for today

.pull-left[
- Refamiliarizing yourself with R

- Basic data transformations

- Missing data
]
.pull-right[
![:scale 70%](images/RStudioCloud_proj_circ.png)
![:scale 70%](images/tidy-1.png)
]


---
class: center, inverse, middle
# For the new class members...

Join the class workspace on RStudio.cloud.
# [http://bit.ly/PSYWorkspace](http://bit.ly/PSYWorkspace)

---
# Tabular data

.pull-left[

```
## # A tibble: 16 x 4
##    Participant Viewpoint Block     RT
##          &lt;int&gt; &lt;fct&gt;     &lt;chr&gt;  &lt;dbl&gt;
##  1           1 Different First   461.
##  2           1 Different Second  450.
##  3           1 Same      First   441.
##  4           1 Same      Second  486.
##  5           2 Different First   448.
##  6           2 Different Second  411.
##  7           2 Same      First   544.
##  8           2 Same      Second  437.
##  9           3 Different First   482.
## 10           3 Different Second  342.
## 11           3 Same      First   496.
## 12           3 Same      Second  443.
## 13           4 Different First   516.
## 14           4 Different Second  486.
## 15           4 Same      First   571.
## 16           4 Same      Second  331.
```
]
.pull-right[
Tables of data are what you're most commonly dealing with in R.

This one confirms to the **tidy data** principles:

One row per observation, one column per variable

![:scale 80%](images/tidy-1.png)
]

---
# Different types of file

The most common file formats you'll deal with are either Excel files or text files, but you may also find dealing with SPSS files useful.

Fortunately, R has several functions and packages for importing data!

|File formats| File extension| Functions| Package|
|-|-|-|-|
|SPSS  | .sav| **read_sav()**| library(haven)|
|Excel | .xls, .xlsx|**read_excel()**|library(readxl)|
|Text  | .csv, .txt, .* |**read_csv()**, **read_delim()**|library(readr)|


---
background-image: url(../images/dplyr-logo.png)
background-size: 6%
background-position: 90% 5%
# Data wrangling

**dplyr** is a really useful package for manipulation of data tables.

|Function |Effect|
|------------|----|
| select()   |Include or exclude variables (columns)|
| arrange()  |Change the order of observations (rows)|
| filter()   |Include or exclude observations (rows)|
| mutate()   |Create new variables (columns)|
| group_by() |Create groups of observations|
| summarise()|Aggregate or summarise groups of observations (rows)|

---
background-image: url(../images/ggplot2-logo.png)
background-size: 8%
background-position: 85% 5%
# Quickly plotting your data

.pull-left[
Plottting data is really important for all aspects of data analysis.

The **ggplot2** package provides a framework for doing this:

```r
ggplot(example_rt_df,
       aes(x = RT, fill = Block)) +
  geom_density(alpha = 0.5) +
  theme_bw()
```
]
.pull-right[
![](Lecture-01-messy-data_files/figure-html/dens-plot-1.png)
]

---
# Running statistics

The humble t-test...


```r
t.test(RT~Block, data = example_rt_df)
```

```
## 
## 	Welch Two Sample t-test
## 
## data:  RT by Block
## t = 15.025, df = 197.04, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##   84.19059 109.63009
## sample estimates:
##  mean in group First mean in group Second 
##             499.9386             403.0283
```

---
# Running statistics

The factorial ANOVA... (the aov_ez() function from the *afex* package)


```r
aov_ez(dv = "RT",
       within = c("Block", "Viewpoint"),
       id = "Participant",
       data = example_rt_df)
```

```
## Anova Table (Type 3 tests)
## 
## Response: RT
##            Effect    df     MSE          F  ges p.value
## 1           Block 1, 49 2313.97 202.93 ***  .53  &lt;.0001
## 2       Viewpoint 1, 49 2197.22       0.71 .004     .40
## 3 Block:Viewpoint 1, 49 1644.91       0.38 .002     .54
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1
```

---
# Reminder! 

Practice materials are available on Datacamp and on RStudio.cloud!

.pull-left[
![:scale 95%](../images/Datacamp_.png)
]
.pull-right[
![:scale 50%](../images/rstudio-primers.png)
]

---
class: center, inverse, middle
# How to handle "messy" or otherwise awkward data

---
# The ideal data

In an ideal world all our data would be beautifully normal:

![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;


---
# The real data

But reality is rarely so kind. Data can be all kinds of messy. It can have *outliers*...

![](Lecture-01-messy-data_files/figure-html/outlier-plot-1.png)&lt;!-- --&gt;

---
# The real data

Data can be *missing*...
.pull-left[
![](Lecture-01-messy-data_files/figure-html/missing-plot-1.png)&lt;!-- --&gt;
]
.pull-right[

```
##           X1        X2
## 1         NA  9.359846
## 2         NA 10.049630
## 3         NA 11.152546
## 4         NA 10.152801
## 5         NA  9.170977
## 6   9.994661        NA
## 7   8.737582        NA
## 8  10.031553  8.265186
## 9  10.460927 12.280524
## 10 10.741676 10.130999
```
Complete cases = 193
]

---
# The real data

Data can be *skewed*...

![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

---
# The real data


.pull-left[
![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]
.pull-right[
There can be any combination of these things...

All of these pose problems for estimating the properties of our data, the relationships between our variables, and the phenomena we are investigating.
]

---
class: center, inverse, middle
# Handling outliers

---
# What is an outlier?

Two out of the 200 pairs of x-y values were replaced. 

The resulting coefficient (approx *r* = .49) is *way-off* the true coefficient for these data. 

.pull-left[
![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
]
.pull-right[
![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;
]

---
# What is an outlier?

.pull-left[
The problem gets even worse with smaller sample sizes.

Here there are 50 datapoints with two outliers, rather than 200.

The correlation coefficient becomes even *more* biased than it was previously.
]

.pull-right[
![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;
]

---
# What should we do with outliers?

Three common approaches:

1. **Remove them**
    
    - If you're sure these reflect an error, not genuine data, then removal is a possibility. 

--
2. **Transformation**
    
    - *rescaling* or *transforming* your data may help reduce the influence of outliers. (We'll come back to this!)

--
3. **Replace them**
    
    - Replacing the outliers with values `\(\pm\)` 2-3 standard deviations away from the mean. 


---
class: center, inverse, middle
# Identifying and replacing outliers

---
# Identifying outliers

Plotting your data can be an excellent way to spot outliers: here they're *very* obvious!

![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;

---
# Identifying outliers

Plotting the residuals of your linear model will also help you identify troublesome observations.

```r
plot(lm(X1 ~ X2, data = temp_df_out))
```
.pull-left[
![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;
]
.pull-right[
![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;
]

---
# Identifying outliers

Plotting the residuals of your linear model will also help you identify troublesome observations.
.pull-left[
![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;
]
.pull-right[
![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;
]

---
# Identifying outliers

.pull-left[
Sometimes a *threshold* is used to determine whether an observation is an outlier.

Sometimes this is driven by common sense: e.g. a value of 120 for a participant's age is **extremely** unlikely to be genuine.

Sometimes this is *data-driven*: e.g. values more than `\(\pm\)` 3 standard deviations away from the mean are *unusual*.
]
.pull-right[
![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;
]

---
# Removing values above a threshold

The **filter()** function from dplyr can be used to remove outliers easily!
.pull-left[

```r
temp_df_out %&gt;%
  ggplot(aes(x = X1, y = X2)) + 
  geom_point() +
  theme_bw() + 
  stat_smooth(method = "lm")
```

![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;
]
.pull-right[

```r
temp_df_out %&gt;%
  dplyr::filter(X1 &lt; 15) %&gt;%
  ggplot(aes(x = X1, y = X2)) + 
  geom_point() + theme_bw() + 
  stat_smooth(method = "lm")
```

![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;
]

---
# Scaling by the mean and standard deviation

The **scale()** function centres a numeric vector on its mean and divides it by its standard deviation.

.pull-left[

```r
example &lt;- c(1:6, 12)
example
```

```
## [1]  1  2  3  4  5  6 12
```

```r
mean(example)
```

```
## [1] 4.714286
```

```r
sd(example)
```

```
## [1] 3.638419
```

]
.pull-right[

```r
scale(example)
```

```
##             [,1]
## [1,] -1.02085147
## [2,] -0.74600684
## [3,] -0.47116222
## [4,] -0.19631759
## [5,]  0.07852704
## [6,]  0.35337166
## [7,]  2.00243942
## attr(,"scaled:center")
## [1] 4.714286
## attr(,"scaled:scale")
## [1] 3.638419
```
]

---
class: center, inverse, middle
# Data transformation

---
# Skewed data

Skewed data is data that *leans* in a particular direction. 

These are often described by the direction of the "long-tail" - so a left-skewed distribution means a distribution with long tail on the left, rather than most values on the left.

![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;

---
# Skewed data

This data is *right-tailed*. This is sometimes also called *positively skewed*. For this type of data, the mean (blue line) is usually higher than the median (red, dashed line).

![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-22-1.png)&lt;!-- --&gt;

---
# Transformation of skewed data

One way to handle this kind of skew is to transform the data onto a different *scale*.

Transformation type| code|
-------|---|
Log|log(X)|
Square root|sqrt(X)|
Reciprocal | 1 / X|

(See Section 5.8.2 in Field et al., DSUR)
---
# Transformation of skewed data

The different transformations each have different effects on the data, but the general pattern is that they (mostly) reduce the skew, bringing extreme values closer to the centre of the distribution.

![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-23-1.png)&lt;!-- --&gt;

---
# Left-tailed skew

Suppose instead that the data is skewed the opposite way - many high scores but few low scores. Thus it has a long *left* tail. This is also called *negative skew*. The mean (blue solid line) is usually less than the median (red dashed line).

![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-24-1.png)&lt;!-- --&gt;

---
# Transformations on left-tailed data

The transformations aren't as effective on left-tailed data.

![](Lecture-01-messy-data_files/figure-html/unnamed-chunk-25-1.png)&lt;!-- --&gt;

---
class: center, inverse, middle
# Handling missing data

---
# Types of missing data

.pull-left[
- Missing Completely At Random
    - Missingness does not depend on anything

- Missing At Random
    - Missingness depends on the observed data

- Missing Not At Random
    - Missingness depends on the missing data
]
.pull-right[
&lt;blockquote class="twitter-tweet"&gt;&lt;p lang="en" dir="ltr"&gt;In today&amp;#39;s lecture, I tried to redefine missing data types (MCAR, MAR, MNAR) as different reasons a dog might eat your homework. This needs more work, but audience seemed to appreciate it. &lt;a href="https://t.co/i9Z8sYrqWQ"&gt;pic.twitter.com/i9Z8sYrqWQ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Richard McElreath (@rlmcelreath) &lt;a href="https://twitter.com/rlmcelreath/status/1101435108995805185?ref_src=twsrc%5Etfw"&gt;March 1, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
]

---
# Missing Completely At Random

.pull-left[
If you have missing data, MCAR is the best kind of missing data. 

There is nothing *systematic* about which data is missing. 

For example, all your participants filled out three different questionnaires. Unfortunately, your dog chewed through a pile of them, and half of your participants now have only two questionnaires.
]
.pull-right[

```
##           X1        X2       X3
## 1         NA 11.196406 14.79229
## 2         NA 11.032400 13.54228
## 3         NA  9.465687 13.37474
## 4         NA  9.575922 11.50504
## 5         NA  9.561335 13.76051
## 6   9.639409  7.802743 11.69513
## 7   8.932981  8.455864 11.81588
## 8   8.464169  8.048474 12.04659
## 9  10.089711  8.164292 12.95979
## 10  7.689520  7.649908 12.41753
```
]

---
# Missing At Random

.pull-left[

```
##           X1        X2       X3 age
## 1  10.126691  7.001005 14.57775  21
## 2   9.235539  9.007883 12.40115  18
## 3   8.991263  9.262714 12.04716  19
## 4   8.570443 10.343828       NA  41
## 5  11.097006  9.979701 14.29012  21
## 6  10.017708  7.838293 12.54091  19
## 7  10.245453  8.855108       NA  26
## 8   9.642139  7.557774 12.71528  20
## 9   9.688923  9.708304 13.27871  20
## 10  8.755107 10.227644 12.56583  18
```
]
.pull-right[
Confusingly, Missing At Random (MAR) data is not missing (completely) at random. 

For example, for some reason, people older than 21 typically failed to complete the third questionnaire.

This data is MAR - whether the data in the third column is missing is related to the value of the fourth column.
]

---
# Missing Not At Random

.pull-left[

```
##           X1        X2       X3 age
## 1   9.808337  9.661388 11.31347  19
## 2  10.657878  8.851631       NA  20
## 3  11.007144  8.629325 13.24104  18
## 4  11.560772  9.475498       NA  19
## 5  10.649922  9.626114 11.07477  19
## 6  10.439654  9.168500 13.01352  20
## 7   9.041356  7.680485 12.45972  21
## 8  11.083638 10.103680 12.77056  18
## 9  10.882921 10.115725       NA  20
## 10 10.002461  8.518686 13.16951  21
```
]
.pull-right[
The final, most troubling type of missing data is data that is Missing Not At Random (MNAR).

For example, imagine that the questionnaire relates to depression; people who score high for depression are less likely to complete the final questionnaire.

In this case, the values that are missing for the third questionnaire depends on the value of the responses to that questionnaire, so this data is MNAR.
]

---
# Simple methods of dealing with missing data

List-wise deletion: Cases with missing data are completely **removed** from **all** analysis.

Pair-wise deletion: Cases with missing data are only **removed** from **comparisons where one or more variables are missing**.

Many R functions have built-in methods of dealing with missing values that enforce the methods. By default, functions such as **mean()** return NA if any value in the input is NA/missing.


```r
mean(temp_df_missing$X1)
```

```
## [1] NA
```

```r
mean(temp_df_missing$X1, na.rm = TRUE)
```

```
## [1] 10.02602
```


---
# Problems with deletion

.pull-left[Deletion can be wasteful. A lot of data is just thrown out. In addition, if the data is Missing Not At Random, this can introduce a lot of bias, since the data is not representative of the full range of the variable of interest!]
.pull-right[
![](Lecture-01-messy-data_files/figure-html/final_cors-1.png)&lt;!-- --&gt;

```
##     X1 X3 X2   
## 157  1  1  1  0
## 21   1  1  0  1
## 11   1  0  1  1
## 11   0  1  1  1
##     11 11 21 43
```
]

---
# Imputation

Imputation is a method of "filling-in" missing values with "best-guesses".

## Single Imputation
Replace missing values with a simple "best-guess". e.g. Using the mean or the median for the condition.

Problem: the mean and median are biased by the missing data. And replacing a missing value with one of these values tends to artificially reduce variability.

## Multiple Imputation
Replace missing values with estimates based on *model* of the data that incorporates uncertainty. We create a model based on the data that is not missing, and use its predictions to guess the values that the missing data has.

Packages such as **mice** and **Amelia** can do this for us.

---
# Alternative approaches to missing data, skew, and other oddities

Generalized Linear Models (as opposed to General Linear Models) allow modelling of data of many different types without necessitating transformations. For example, counts can be modelled using Poisson regression, and categorical outcomes with logistic regression.

Multilevel, hierarchial models can handle all of these things and much more besides; they are perfectly capable of handling missing data.

We'll cover both of these later in the course.


---
# Next week

Look into power and effect sizes:

See Field et al, Discovering Statistics Using R, pages 56-59, Sections on:
    - Type I and Type II error (2.6.3)
    - effect sizes (2.6.4)
    - statistical power (2.6.5)

Cohen, J. (1992). A power primer. Psychological Bulletin, 112(1), 155-159.
http://dx.doi.org/10.1037/0033-2909.112.1.155
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="js/macros.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
