<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Multiple and logistic regression</title>
    <meta charset="utf-8" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/countdown/countdown.css" rel="stylesheet" />
    <script src="libs/countdown/countdown.js"></script>
    <link rel="stylesheet" href="css\my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Multiple and logistic regression
## Week 26 - PSY9003
### 2020/03/17

---




class: inverse, middle, center
# Linear regression - a recap

---
# Linear regression

.pull-left[

![](Week-26-generalized-linear-models_files/figure-html/examp-lm-1.png)
]
.pull-right[
Our job is to figure out the mathematical relationship between our *predictor(s)* and our *outcome*.

$$ Y = b_0 + b_1X_i + \varepsilon_i $$
]

---
# Linear regression

$$ Y = b_0 + b_1X_i + \varepsilon_i $$

---
# Linear regression

$$ \color{red}Y = \color{#F6893D}{b_0} + \color{#FEBC38}{b_1}\color{blue}{X_i} + \color{green}{\varepsilon_i} $$

.red[
Y - The outcome - the dependent variable.
]

.brown[
`\(b_0\)` - The *intercept*. This is the value of `\(Y\)` when `\(X\)` = 0.
]

.orange[
`\(b_1\)` - The regression coefficients. This describes the steepness of the relationship between the outcome and *slope(s)*.
]

.blue[
`\(X_i\)` - The predictors - our independent variables.
]

.green[
`\(\varepsilon_i\)` - The *random error* - variability in our dependent variable that is not explained by our predictors. 
]


---
class: inverse, middle, center
# Regression assumptions

---
# Regression assumptions

Linear regression has a number of assumptions.

- Normally distributed errors

- Homoscedasticity (of errors)

- Independence of errors

- Linearity

- No perfect multicollinearity

---
# Normally distributed errors

.pull-left[
![](Week-26-generalized-linear-models_files/figure-html/unnamed-chunk-1-1.png)&lt;!-- --&gt;
]

.pull-right[
The *errors*, `\(\varepsilon_i\)`, are the variance left over after your model is fit.

An example like that on the left is what you want to see!

There is no clear pattern; the dots are evenly distributed around zero.

]

---
# Skewed errors

.pull-left[
![](Week-26-generalized-linear-models_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;
]
.pull-right[
In contrast, the residuals on the left are skewed.

This most often happens with data that are *bounded*. For example, *reaction times* cannot be below zero; negative values are impossible.
]

---
class: center, middle, inverse
# Generalized Linear Models

---
# Generalized linear models

So far we've been using the **General Linear Model** (using the **lm()** function).

However, there is a broader class called **Generalized Linear Models**, of which the *General Linear Model* is a sub-class.

A Generalized Linear Model allows use of linear regression for different types of data by *projecting the data onto an appropriate scale*.

---
# Categorical outcome variables

Suppose you have a *discrete*, *categorical* outcome.

Examples of categorical outcomes:
- correct or incorrect answer
- person commits an offence or does not

Examples of counts:
- Number of items succesfully recalled
- Number of offences committed

---
# The binomial distribution

A coin only has two sides: heads or tails.

Assuming the coin is fair, the probability - `\(P\)` - that it lands on *heads* is *.5*. So the probability it lands on *tails* - `\(1 - P\)` is also *.5*.

This type of variable is called a **Bernoulli random variable**.

If you toss the coin many times, the count of how many heads and how many tails occur is called a **binomial distribution**.

---
# Binomial distribution

If we throw the coin 100 times, how many times do we get tails?


```r
qplot(rbinom(100, 1, 0.5))
```

![](Week-26-generalized-linear-models_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;


---
# Binomial distribution

If we throw the coin five times 100 times, how many tails do we get each time?


```r
qplot(rbinom(100, 5, 0.5))
```

![](Week-26-generalized-linear-models_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

---
# Generalized linear models

To model data such as these, we need to use *Generalized linear models*.

The *glm()* function in R can be used to fit such models.

.pull-left[

```r
lm(Y ~ X1)
```

```
## 
## Call:
## lm(formula = Y ~ X1)
## 
## Coefficients:
## (Intercept)           X1  
##     0.07472     -0.04452
```
]
.pull-right[

```r
glm(Y ~ X1, family = "gaussian")
```

```
## 
## Call:  glm(formula = Y ~ X1, family = "gaussian")
## 
## Coefficients:
## (Intercept)           X1  
##     0.07472     -0.04452  
## 
## Degrees of Freedom: 299 Total (i.e. Null);  298 Residual
## Null Deviance:	    473.4 
## Residual Deviance: 472.8 	AIC: 993.8
```
]

---
# Distributional families
The *family* argument to **glm()** allows you to specify what type of probability distribution the data is drawn from.

.pull-left[
![](Week-26-generalized-linear-models_files/figure-html/hypothetical-norm-1.png)&lt;!-- --&gt;
]
.pull-right[
The *normal* distribution can also be called the *Gaussian* distribution.

The linear regression models we've used so far assume a *Gaussian* distribution of errors.
]

---
# Logistic regression

We can model a binomial distribution by specifying the *binomial* family.



```r
coin_flips &lt;- rbinom(n = 200, size = 1, prob = 0.5)
glm(coin_flips ~ 1,
    family = binomial(link = "logit"))
```

```
## 
## Call:  glm(formula = coin_flips ~ 1, family = binomial(link = "logit"))
## 
## Coefficients:
## (Intercept)  
##       -0.02  
## 
## Degrees of Freedom: 199 Total (i.e. Null);  199 Residual
## Null Deviance:	    277.2 
## Residual Deviance: 277.2 	AIC: 279.2
```

---
# Logistic regression

Importantly, we also specify here a *link* function - the "logit". 


```r
glm(coin_flips ~ 1, family = binomial(link = "logit"))
```

```
## 
## Call:  glm(formula = coin_flips ~ 1, family = binomial(link = "logit"))
## 
## Coefficients:
## (Intercept)  
##       -0.02  
## 
## Degrees of Freedom: 199 Total (i.e. Null);  199 Residual
## Null Deviance:	    277.2 
## Residual Deviance: 277.2 	AIC: 279.2
```


(note - this is the *default*, and is what will be used if you simply type family = binomial())

---
# Logistic regression

.pull-left[
The *logit* transformation is used to *link* our predictors to our discrete outcome variable.

It helps us constrain the influence of our predictors to the range 0-1, and account for the change in *variance* with probability.
]
.pull-right[
![](Week-26-generalized-linear-models_files/figure-html/logit-curve-1.png)&lt;!-- --&gt;
]

---
# Logistic regression

.pull-left[
As probabilities approach zero or one, the range of possible values *decreases*.

Thus, the influence of predictors on the *response scale* must also decrease as we reach one or zero.

]
.pull-right[
![](Week-26-generalized-linear-models_files/figure-html/logit-curve-1.png)
]

---
# Logistic regression

$$ \color{red}{P(Y)} = \frac{1}{1 + e^-(b_0 + b_1X_1 + \varepsilon_i)} $$

.red[
`\(P(Y)\)` - The *probability* of the outcome happening.
]

---
# Logistic regression

$$ \color{red}{P(Y)} = \frac{\color{green}{1}}{\color{green}{1 + e^-(b_0 + b_1X_1 + \varepsilon_i)}} $$

.red[
`\(P(Y)\)` - The *probability* of the outcome happening.
]
.green[
`\(\frac{1}{1 + e^-(...)}\)` - The *log-odds* (logits) of the predictors.
]

---
# Odds and log odds

**Odds** are the ratio of one outcome versus the others. e.g. The odds of a randomly chosen day being a Friday are 1 to 6 (or 1/6 = .17)

**Log odds** are the *natural log* of the odds:

`$$log(\frac{p}{1-p})$$`


```r
coef(glm(coin_flips ~ 1, family = binomial(link = "logit")))
```

```
## (Intercept) 
## -0.02000067
```


---
class: inverse, middle, center
# Taking penalties

--

![](images/terry-penalty.gif)

---
# Taking penalties

What's the probability that a particular penalty will be scored?


```
##   PSWQ Anxious Previous         Scored
## 1   18      21       56 Scored Penalty
## 2   17      32       35 Scored Penalty
## 3   16      34       35 Scored Penalty
## 4   14      40       15 Scored Penalty
## 5    5      24       47 Scored Penalty
## 6    1      15       67 Scored Penalty
```

- PSWQ = Penn State Worry Questionnaire
- Anxiety = State Anxiety
- Previous = Number of penalties scored previously

---
# Taking penalties



```r
pens &lt;- glm(Scored ~ PSWQ + Anxious + Previous,
            family = binomial(link = "logit"),
            data = penalties)
pens
```

```
## 
## Call:  glm(formula = Scored ~ PSWQ + Anxious + Previous, family = binomial(link = "logit"), 
##     data = penalties)
## 
## Coefficients:
## (Intercept)         PSWQ      Anxious     Previous  
##    -11.4926      -0.2514       0.2758       0.2026  
## 
## Degrees of Freedom: 74 Total (i.e. Null);  71 Residual
## Null Deviance:	    103.6 
## Residual Deviance: 47.42 	AIC: 55.42
```

---


```
## 
## Call:
## glm(formula = Scored ~ PSWQ + Anxious + Previous, family = binomial(link = "logit"), 
##     data = penalties)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.31374  -0.35996   0.08334   0.53860   1.61380  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept) -11.49256   11.80175  -0.974  0.33016   
*## PSWQ         -0.25137    0.08401  -2.992  0.00277 **
## Anxious       0.27585    0.25259   1.092  0.27480   
## Previous      0.20261    0.12932   1.567  0.11719   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 103.638  on 74  degrees of freedom
## Residual deviance:  47.416  on 71  degrees of freedom
## AIC: 55.416
## 
## Number of Fisher Scoring iterations: 6
```

---
# The response scale and the link scale

The model is fit on the *link* scale. 

The coefficients returned by the GLM are in *logits*, or *log-odds*.


```r
coef(pens)
```

```
## (Intercept)        PSWQ     Anxious    Previous 
## -11.4925608  -0.2513693   0.2758489   0.2026082
```

How do we interpret them?

---
# Converting logits to odds ratios


```r
coef(pens)[2:4]
```

```
##       PSWQ    Anxious   Previous 
## -0.2513693  0.2758489  0.2026082
```

We can *exponentiate* the log-odds using the **exp()** function.


```r
exp(coef(pens)[2:4])
```

```
##      PSWQ   Anxious  Previous 
## 0.7777351 1.3176488 1.2245925
```

---
# Odds ratios

An odds ratio greater than 1 means that the odds of an outcome increase.

An odds ratio less than 1 means that the odds of an outcome decrease.


```r
exp(coef(pens)[2:4])
```

```
##      PSWQ   Anxious  Previous 
## 0.7777351 1.3176488 1.2245925
```

From this table, it looks like the odds of scoring a penalty decrease with increases in PSWQ but increase with increases in State Anxiety or Previous scoring rates.

---
# The response scale 

The *response* scale is even *more* intuitive: it is back in the *original* units. We can generate probabilities using the **predict()** function.


```r
penalties$prob &lt;- predict(pens, type = "response")
head(penalties)
```

```
##   PSWQ Anxious Previous         Scored      prob
## 1   18      21       56 Scored Penalty 0.7542999
## 2   17      32       35 Scored Penalty 0.5380797
## 3   16      34       35 Scored Penalty 0.7222563
## 4   14      40       15 Scored Penalty 0.2811731
## 5    5      24       47 Scored Penalty 0.9675024
## 6    1      15       67 Scored Penalty 0.9974486
```

---
# Model predictions

Note that these model predictions *don't need to use the original data*.


```r
new_dat &lt;- tibble::tibble(PSWQ = seq(0, 30, by = 2),
                          Anxious = mean(penalties$Anxious),
                          Previous = mean(penalties$Previous))
new_dat$probs &lt;- predict(pens,
                         newdata = new_dat,
                         type = "response")
plot(new_dat$PSWQ, new_dat$probs)
```

![](Week-26-generalized-linear-models_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;

---
# Model predictions

Imagine you wanted to the probability of scoring for somebody with a PSWQ score of 7, an Anxious rating of 12, and a Previous scoring record of 34:


```r
new_dat &lt;- tibble::tibble(PSWQ = 7,
                          Anxious = 22,
                          Previous = 34)
predict(pens, new_dat, type = "response")
```

```
##         1 
## 0.4268314
```

.pull-left[

```r
predict(pens, new_dat)
```

```
##          1 
## -0.2947909
```
]
.pull-right[

```r
exp(predict(pens, new_dat))
```

```
##         1 
## 0.7446873
```
]

---
# Plotting

Often the best way to understand the results is to *plot* them!
.pull-left[

The **sjPlot** package has some excellent built in plotting tools - try the **plot_model()** function.


```r
library(sjPlot)
plot_model(pens,
           type = "pred",
           terms = "PSWQ")
```

```
## Data were 'prettified'. Consider using `terms="PSWQ [all]"` to get smooth plots.
```
]
.pull-right[
![](Week-26-generalized-linear-models_files/figure-html/sj-plot-1.png)
]

---
# Plotting

Note that this is a plot of the *predicted probabilities*!

.pull-left[

```r
ggplot(penalties,
       aes(x = PSWQ, y = prob)) +
  geom_point() + 
  geom_smooth(method = "glm",
              method.args = list(family = binomial()))
```
]
.pull-right[
![](Week-26-generalized-linear-models_files/figure-html/pred-gg-1.png)
]

---
# Results tables


```r
sjPlot::tab_model(pens)
```

&lt;table style="border-collapse:collapse; border:none;"&gt;
&lt;tr&gt;
&lt;th style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; "&gt;&amp;nbsp;&lt;/th&gt;
&lt;th colspan="3" style="border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; "&gt;Scored&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; "&gt;Predictors&lt;/td&gt;
&lt;td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  "&gt;Odds Ratios&lt;/td&gt;
&lt;td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  "&gt;CI&lt;/td&gt;
&lt;td style=" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  "&gt;p&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; "&gt;(Intercept)&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;0.00&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;0.00&amp;nbsp;&amp;ndash;&amp;nbsp;64258.63&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;0.330&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; "&gt;PSWQ&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;0.78&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;0.64&amp;nbsp;&amp;ndash;&amp;nbsp;0.90&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;&lt;strong&gt;0.003&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; "&gt;Anxious&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;1.32&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;0.81&amp;nbsp;&amp;ndash;&amp;nbsp;2.24&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;0.275&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; "&gt;Previous&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;1.22&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;0.96&amp;nbsp;&amp;ndash;&amp;nbsp;1.61&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  "&gt;0.117&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;"&gt;Observations&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;" colspan="3"&gt;75&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;"&gt;R&lt;sup&gt;2&lt;/sup&gt; Tjur&lt;/td&gt;
&lt;td style=" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;" colspan="3"&gt;0.594&lt;/td&gt;
&lt;/tr&gt;

&lt;/table&gt;

---
class: inverse, middle, center
# Main Exercise 

<div class="countdown" id="timer_5e74e45b" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">15</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---
class: inverse, middle, center
# The Titanic dataset

![](images/titanic.gif)

---
class: inverse, middle, center
# The Titanic dataset

![](images/rose84.gif)
---
# The Titanic dataset




```r
head(titanic)
```

```
## # A tibble: 6 x 12
##   PassengerId Survived Pclass Name  Sex     Age SibSp Parch Ticket  Fare Cabin
##         &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;
## 1           1        0      3 Brau~ male     22     1     0 A/5 2~  7.25 &lt;NA&gt; 
## 2           2        1      1 Cumi~ fema~    38     1     0 PC 17~ 71.3  C85  
## 3           3        1      3 Heik~ fema~    26     0     0 STON/~  7.92 &lt;NA&gt; 
## 4           4        1      1 Futr~ fema~    35     1     0 113803 53.1  C123 
## 5           5        0      3 Alle~ male     35     0     0 373450  8.05 &lt;NA&gt; 
## 6           6        0      3 Mora~ male     NA     0     0 330877  8.46 &lt;NA&gt; 
## # ... with 1 more variable: Embarked &lt;chr&gt;
```

https://www.kaggle.com/c/titanic

---
# The Titanic dataset

![](images/titanic.png)

---
# The Titanic dataset

.pull-left[

```r
titanic %&gt;% 
  group_by(Survived,
           Sex) %&gt;%
  count()
```

```
## # A tibble: 4 x 3
## # Groups:   Survived, Sex [4]
##   Survived Sex        n
##      &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;
## 1        0 female    81
## 2        0 male     468
## 3        1 female   233
## 4        1 male     109
```
]
.pull-right[

```r
titanic %&gt;%
  group_by(Sex) %&gt;%
  summarise(p = mean(Survived),
            Y = sum(Survived),
            N = n())
```

```
## # A tibble: 2 x 4
##   Sex        p     Y     N
##   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
## 1 female 0.742   233   314
## 2 male   0.189   109   577
```
]

---
# The Titanic dataset


```
## 
## Call:
## glm(formula = Survived ~ Age + Pclass, family = binomial(), data = titanic)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1524  -0.8466  -0.6083   1.0031   2.3929  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  2.296012   0.317629   7.229 4.88e-13 ***
## Age         -0.041755   0.006736  -6.198 5.70e-10 ***
## Pclass2     -1.137533   0.237578  -4.788 1.68e-06 ***
## Pclass3     -2.469561   0.240182 -10.282  &lt; 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 827.16  on 710  degrees of freedom
##   (177 observations deleted due to missingness)
## AIC: 835.16
## 
## Number of Fisher Scoring iterations: 4
```

---
# The Titanic dataset


```r
library(emmeans)
emmeans(age_class, ~Age|Pclass, type = "response")
```

```
## Pclass = 1:
##   Age  prob     SE  df asymp.LCL asymp.UCL
##  29.7 0.742 0.0339 Inf     0.670     0.803
## 
## Pclass = 2:
##   Age  prob     SE  df asymp.LCL asymp.UCL
##  29.7 0.480 0.0394 Inf     0.403     0.557
## 
## Pclass = 3:
##   Age  prob     SE  df asymp.LCL asymp.UCL
##  29.7 0.196 0.0216 Inf     0.157     0.241
## 
## Confidence level used: 0.95 
## Intervals are back-transformed from the logit scale
```

---
# Some final notes on Generalized Linear Models

Today has focussed on **logistic** regression with *binomial* distributions.

But Generalized Linear Models can be expanded to deal with many different types of outcome variable!

e.g. 
*Counts* follow a Poisson distribution - use `family = "poisson"`

Ordinal variables (e.g. Likert scale) can be modelled using *cumulative logit* models (using the **ordinal** or **brms** packages).

---
#Suggested reading for categorical ordinal regression

[Liddell &amp; Kruschke (2018). Analyzing ordinal data with metric models: What could possibly go Wrong?](https://www.sciencedirect.com/science/article/pii/S0022103117307746)

[Buerkner &amp; Vuorre (2018). Ordinal Regression Models in Psychology: A Tutorial](https://psyarxiv.com/x8swp/)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="js/macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
